{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991cf177",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "This notebook trains the deep neural network for multi-layer channel estimation.  \n",
    "An **already-trained model** is provided in the `Models` directory. A comprehensive hyperparameter search was performed to obtain this model. You may use this notebook to train your own model or proceed directly to evaluation using the included model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90d6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ChEstNet import ChEstNet, ChEstDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfa5fe8-e489-4612-bc5c-5c775a132c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets:\n",
    "dataPath = \"/data/datasets/SelfRefine\"      # Replace with the location of your data files\n",
    "batchSize = 64                             \n",
    "trainDS = ChEstDataset( os.path.join(dataPath,\"Train.npy\"), batchSize )\n",
    "validDS = ChEstDataset( os.path.join(dataPath,\"Valid.npy\"), batchSize )\n",
    "testDS = ChEstDataset( os.path.join(dataPath,\"Test.npy\"), batchSize )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258684bd-7f88-4252-8066-d6dc7bb58ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFileName = \"Models/Trained.pth\"        # The model file name after the training\n",
    "numEpochs = 100\n",
    "lrStart, lrEnd = 0.002, 0.00002             # Learning rate exponentially decaying from 'lrStart' to 'lrEnd'\n",
    "\n",
    "device = f\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = ChEstNet(device)                    # Create the model object\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrStart)\n",
    "lrScheduler = ExponentialLR(optimizer, np.exp(np.log(lrEnd/lrStart)/(numEpochs-1)))\n",
    "lossFunction = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e15fa01-7c90-4303-a674-8b1bceb497d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   Learning Rate   Training Loss   Validation Loss\n",
      "-----   -------------   -------------   ---------------\n",
      "    1       0.002000        0.592596        0.350749   \n",
      "    2       0.001909        0.286578        0.293276 * \n",
      "    3       0.001822        0.229406        0.181759 * \n",
      "    4       0.001739        0.201478        0.238252   \n",
      "    5       0.001660        0.185287        0.170843 * \n",
      "    6       0.001585        0.169945        0.162267 * \n",
      "    7       0.001513        0.161321        0.140687 * \n",
      "    8       0.001444        0.153775        0.134481 * \n",
      "    9       0.001379        0.147432        0.120986 * \n",
      "   10       0.001316        0.141432        0.117913 * \n",
      "   11       0.001256        0.137113        0.124352   \n",
      "   12       0.001199        0.132559        0.126296   \n",
      "   13       0.001144        0.129277        0.114175 * \n",
      "   14       0.001092        0.126393        0.104231 * \n",
      "   15       0.001043        0.122861        0.099872 * \n",
      "   16       0.000995        0.119728        0.100929   \n",
      "   17       0.000950        0.118056        0.102004   \n",
      "   18       0.000907        0.117423        0.104920   \n",
      "   19       0.000866        0.115370        0.098371 * \n",
      "   20       0.000826        0.113268        0.101151   \n",
      "   21       0.000789        0.112372        0.100470   \n",
      "   22       0.000753        0.111739        0.093896 * \n",
      "   23       0.000719        0.110167        0.098650   \n",
      "   24       0.000686        0.108182        0.092074 * \n",
      "   25       0.000655        0.107351        0.094154   \n",
      "   26       0.000625        0.107393        0.095784   \n",
      "   27       0.000597        0.106233        0.093463   \n",
      "   28       0.000570        0.105029        0.090312 * \n",
      "   29       0.000544        0.104499        0.094769   \n",
      "   30       0.000519        0.103437        0.092911   \n",
      "   31       0.000495        0.103169        0.094986   \n",
      "   32       0.000473        0.102576        0.090984   \n",
      "   33       0.000451        0.101915        0.092137   \n",
      "   34       0.000431        0.101388        0.089952 * \n",
      "   35       0.000411        0.101063        0.090796   \n",
      "   36       0.000393        0.100525        0.089677 * \n",
      "   37       0.000375        0.100141        0.088886 * \n",
      "   38       0.000358        0.099775        0.088922   \n",
      "   39       0.000341        0.099053        0.087850 * \n",
      "   40       0.000326        0.098822        0.090153   \n",
      "   41       0.000311        0.098318        0.088497   \n",
      "   42       0.000297        0.097601        0.089126   \n",
      "   44       0.000271        0.097334        0.089537   \n",
      "   45       0.000258        0.097037        0.086837 * \n",
      "   46       0.000247        0.096969        0.086701 * \n",
      "   47       0.000235        0.096301        0.087230   \n",
      "   48       0.000225        0.096416        0.088820   \n",
      "   49       0.000214        0.095824        0.087847   \n",
      "   50       0.000205        0.095676        0.087539   \n",
      "   51       0.000195        0.095642        0.086658 * \n",
      "   52       0.000187        0.095697        0.088215   \n",
      "   53       0.000178        0.095039        0.086919   \n",
      "   54       0.000170        0.095196        0.085815 * \n",
      "   55       0.000162        0.094859        0.086339   \n",
      "   56       0.000155        0.094708        0.086105   \n",
      "   57       0.000148        0.094466        0.085681 * \n",
      "   58       0.000141        0.094248        0.086342   \n",
      "   59       0.000135        0.094065        0.086137   \n",
      "   60       0.000129        0.094064        0.085811   \n",
      "   61       0.000123        0.093676        0.085701   \n",
      "   62       0.000117        0.093895        0.085013 * \n",
      "   63       0.000112        0.093577        0.085678   \n",
      "   64       0.000107        0.093525        0.085168   \n",
      "   65       0.000102        0.093349        0.084684 * \n",
      "   66       0.000097        0.093187        0.085046   \n",
      "   67       0.000093        0.093136        0.084670 * \n",
      "   68       0.000089        0.093030        0.084991   \n",
      "   69       0.000085        0.093028        0.085130   \n",
      "   70       0.000081        0.093039        0.084782   \n",
      "   71       0.000077        0.092802        0.084764   \n",
      "   72       0.000074        0.092709        0.084589 * \n",
      "   73       0.000070        0.092619        0.084323 * \n",
      "   74       0.000067        0.092628        0.084564   \n",
      "   75       0.000064        0.092514        0.084806   \n",
      "   76       0.000061        0.092381        0.084581   \n",
      "   77       0.000058        0.092428        0.084450   \n",
      "   78       0.000056        0.092437        0.084673   \n",
      "   79       0.000053        0.092257        0.084661   \n",
      "   80       0.000051        0.092136        0.084257 * \n",
      "   81       0.000048        0.092143        0.084315   \n",
      "   82       0.000046        0.092017        0.084559   \n",
      "   83       0.000044        0.092061        0.084437   \n",
      "   84       0.000042        0.092222        0.084414   \n",
      "   85       0.000040        0.091911        0.084190 * \n",
      "   86       0.000038        0.091995        0.084767   \n",
      "   87       0.000037        0.092073        0.084218   \n",
      "   88       0.000035        0.092001        0.084403   \n",
      "   89       0.000033        0.091661        0.084257   \n",
      "   90       0.000032        0.091813        0.084155 * \n",
      "   91       0.000030        0.091706        0.084101 * \n",
      "   92       0.000029        0.091669        0.084034 * \n",
      "   93       0.000028        0.091745        0.084215   \n",
      "   94       0.000026        0.091648        0.084087   \n",
      "   95       0.000025        0.091672        0.084165   \n",
      "   96       0.000024        0.091667        0.084164   \n",
      "   97       0.000023        0.091507        0.084043   \n",
      "   98       0.000022        0.091636        0.084170   \n",
      "   99       0.000021        0.091591        0.084497   \n",
      "  100       0.000020        0.091596        0.084509   \n"
     ]
    }
   ],
   "source": [
    "# Main Training Loop:\n",
    "lowestLoss, bestEpoch = None, None\n",
    "validLoss = None\n",
    "print(\"Epoch   Learning Rate   Training Loss   Validation Loss\")\n",
    "print(\"-----   -------------   -------------   ---------------\")\n",
    "for epoch in range(numEpochs):\n",
    "    curLr = lrScheduler.get_last_lr()[0]\n",
    "    print(f\" {epoch+1:-4d}     {curLr:-10f}      \", end=\"\")\n",
    "        \n",
    "    # Train one epoch\n",
    "    lossMin, lossMean, lossMax = model.trainEpoch(trainDS, lossFunction, optimizer)\n",
    "    print(f\"{lossMean:-10f}      \", end=\"\")\n",
    "    \n",
    "    validLoss = model.evaluate(validDS, lossFunction)\n",
    "    if lowestLoss is None:\n",
    "        lowestLoss, bestEpoch = validLoss, epoch+1\n",
    "        model.saveParams(modelFileName)     # Save the best model so far \n",
    "        print(f\"{validLoss:-10f}   \")\n",
    "    elif validLoss<lowestLoss:\n",
    "        lowestLoss, bestEpoch = validLoss, epoch+1\n",
    "        model.saveParams(modelFileName)     # Save the best model so far \n",
    "        print(f\"{validLoss:-10f} * \")\n",
    "    else:\n",
    "        print(f\"{validLoss:-10f}   \")\n",
    "    lrScheduler.step()\n",
    "\n",
    "model.loadParams(modelFileName)             # Load the best model\n",
    "validLoss = lowestLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2078a9-7a13-4371-bd12-49fd282400bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b8ff9-91ae-4f01-a687-82df5aeedaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
